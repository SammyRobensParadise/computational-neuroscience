{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Reinforcement Learning Mouse Model of Maze Discovery\n",
        "\n",
        "SYDE 552\n",
        "\n",
        "Winter 2023\n",
        "\n",
        "April 21st 2023\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Importing neccesary libraries for data creation and visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cuygndk_FToF"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-17 12:28:06.786973: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Activation\n",
        "from keras.optimizers import SGD , Adam, RMSprop\n",
        "from keras.layers.activation import PReLU\n",
        "from random import randint\n",
        "import os, sys, time, datetime, json, random"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Code for simulating a rat in a maze with actions, agent and reward\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "twcMVfZlGkKv"
      },
      "outputs": [],
      "source": [
        "# dcreate the colors\n",
        "visited_mark = 0.8  # Cells visited by the rat will be painted by gray 0.8\n",
        "mouse_mark = 0.5  # The current rat cell will be painteg by gray 0.5\n",
        "\n",
        "# numerically assign valus to possible actions\n",
        "# assume rat cannot move diagonal\n",
        "LEFT = 0\n",
        "UP = 1\n",
        "RIGHT = 2\n",
        "DOWN = 3\n",
        "\n",
        "# Actions dictionary\n",
        "actions_dict: dict[int, str] = {\n",
        "    LEFT: \"left\",\n",
        "    UP: \"up\",\n",
        "    RIGHT: \"right\",\n",
        "    DOWN: \"down\",\n",
        "}\n",
        "\n",
        "num_actions: int = len(actions_dict)\n",
        "\n",
        "# Exploration factor\n",
        "#  one of every 10 moves the agent takes a completely random action\n",
        "epsilon: float = 1 / 10\n",
        "\n",
        "\n",
        "# maze is a 2d Numpy array of floats between 0.0 to 1.0\n",
        "# 1.0 corresponds to a free cell, and 0.0 an occupied cell\n",
        "# mouse = (row, col) initial mouse position (defaults to (0,0))\n",
        "\n",
        "\n",
        "class Qmaze(object):\n",
        "    def __init__(\n",
        "        self,\n",
        "        maze: list,\n",
        "        mouse: list = (0, 0),\n",
        "        valid_penalty: float = -0.04,\n",
        "        invalid_penality: float = -0.75,\n",
        "        visited_penality: float = -0.25,\n",
        "    ):\n",
        "        self._maze = np.array(maze)\n",
        "        nrows, ncols = self._maze.shape\n",
        "        self._valid_penality = valid_penalty\n",
        "        self._invalid_penality = invalid_penality\n",
        "        self._visited_penality = visited_penality\n",
        "\n",
        "        # target cell where the \"cheese\" is\n",
        "        # the default behaviour is that the cheese is always in the\n",
        "        # bottom right corner of the maze\n",
        "        self.target = (nrows - 1, ncols - 1)\n",
        "\n",
        "        # create free cells\n",
        "        self.free_cells = [\n",
        "            (r, c)\n",
        "            for r in range(nrows)\n",
        "            for c in range(ncols)\n",
        "            if self._maze[r, c] == 1.0\n",
        "        ]\n",
        "        # remove the target from the \"free cells\"\n",
        "        self.free_cells.remove(self.target)\n",
        "\n",
        "        # throw an exception if there is no way to get to the target cell\n",
        "        if self._maze[self.target] == 0.0:\n",
        "            raise Exception(\"Invalid maze: target cell cannot be blocked!\")\n",
        "\n",
        "        # throw an exception if the mouse is not started on a free cell\n",
        "        if not mouse in self.free_cells:\n",
        "            raise Exception(\"Invalid mouse Location: must sit on a free cell\")\n",
        "        self.reset(mouse)\n",
        "\n",
        "    def reset(self, mouse):\n",
        "        self.mouse = mouse\n",
        "        self.maze = np.copy(self._maze)\n",
        "        _, _ = self.maze.shape\n",
        "        row, col = mouse\n",
        "        self.maze[row, col] = mouse_mark\n",
        "        self.state = (row, col, \"start\")\n",
        "        self.min_reward = -0.5 * self.maze.size\n",
        "        self.total_reward = 0\n",
        "        self.visited = set()\n",
        "\n",
        "    def update_state(self, action):\n",
        "        _, _ = self.maze.shape\n",
        "        nrow, ncol, nmode = mouse_row, mouse_col, mode = self.state\n",
        "\n",
        "        if self.maze[mouse_row, mouse_col] > 0.0:\n",
        "            self.visited.add((mouse_row, mouse_col))  # mark visited cell\n",
        "\n",
        "        valid_actions = self.valid_actions()\n",
        "\n",
        "        if not valid_actions:\n",
        "            nmode = \"blocked\"\n",
        "        elif action in valid_actions:\n",
        "            nmode = \"valid\"\n",
        "            if action == LEFT:\n",
        "                ncol -= 1\n",
        "            elif action == UP:\n",
        "                nrow -= 1\n",
        "            if action == RIGHT:\n",
        "                ncol += 1\n",
        "            elif action == DOWN:\n",
        "                nrow += 1\n",
        "        else:  # invalid action, no change mouse position\n",
        "            mode = \"invalid\"\n",
        "\n",
        "        # new state\n",
        "        self.state = (nrow, ncol, nmode)\n",
        "\n",
        "    def get_reward(self):\n",
        "        mouse_row, mouse_col, mode = self.state\n",
        "        nrows, ncols = self.maze.shape\n",
        "        valid_penalty = self._valid_penality\n",
        "        invalid_penalty = self._invalid_penality\n",
        "        visited_penalty = self._visited_penality\n",
        "        if mouse_row == nrows - 1 and mouse_col == ncols - 1:\n",
        "            return 1.0\n",
        "        if mode == \"blocked\":\n",
        "            return self.min_reward - 1\n",
        "        if (mouse_row, mouse_col) in self.visited:\n",
        "            return visited_penalty\n",
        "        if mode == \"invalid\":\n",
        "            return invalid_penalty\n",
        "        if mode == \"valid\":\n",
        "            return valid_penalty\n",
        "\n",
        "    def act(self, action):\n",
        "        self.update_state(action)\n",
        "        reward = self.get_reward()\n",
        "        self.total_reward += reward\n",
        "        status = self.trial_status()\n",
        "        envstate = self.observe()\n",
        "        return envstate, reward, status\n",
        "\n",
        "    def observe(self):\n",
        "        canvas = self.create_environment()\n",
        "        envstate = canvas.reshape((1, -1))\n",
        "        return envstate\n",
        "\n",
        "    def create_environment(self):\n",
        "        canvas = np.copy(self.maze)\n",
        "        nrows, ncols = self.maze.shape\n",
        "        # clear all visual marks\n",
        "        for r in range(nrows):\n",
        "            for c in range(ncols):\n",
        "                if canvas[r, c] > 0.0:\n",
        "                    canvas[r, c] = 1.0\n",
        "        # draw the mouse\n",
        "        row, col, valid = self.state\n",
        "        canvas[row, col] = mouse_mark\n",
        "        return canvas\n",
        "\n",
        "    def trial_status(self):\n",
        "        if self.total_reward < self.min_reward:\n",
        "            return \"lose\"\n",
        "        mouse_row, mouse_col, mode = self.state\n",
        "        nrows, ncols = self.maze.shape\n",
        "        if mouse_row == nrows - 1 and mouse_col == ncols - 1:\n",
        "            return \"win\"\n",
        "\n",
        "        return \"not_over\"\n",
        "\n",
        "    def valid_actions(self, cell=None):\n",
        "        if cell is None:\n",
        "            row, col, _ = self.state\n",
        "        else:\n",
        "            row, col = cell\n",
        "        actions = [0, 1, 2, 3]\n",
        "        nrows, ncols = self.maze.shape\n",
        "        if row == 0:\n",
        "            actions.remove(1)\n",
        "        elif row == nrows - 1:\n",
        "            actions.remove(3)\n",
        "\n",
        "        if col == 0:\n",
        "            actions.remove(0)\n",
        "        elif col == ncols - 1:\n",
        "            actions.remove(2)\n",
        "\n",
        "        if row > 0 and self.maze[row - 1, col] == 0.0:\n",
        "            actions.remove(1)\n",
        "        if row < nrows - 1 and self.maze[row + 1, col] == 0.0:\n",
        "            actions.remove(3)\n",
        "\n",
        "        if col > 0 and self.maze[row, col - 1] == 0.0:\n",
        "            actions.remove(0)\n",
        "        if col < ncols - 1 and self.maze[row, col + 1] == 0.0:\n",
        "            actions.remove(2)\n",
        "\n",
        "        return actions\n",
        "\n",
        "\n",
        "# show 8x8 maze | WALL = BLACK | MOUSE = DARK GRAY | PATH = LIGHT GRAY | CHEESE = VERY LIGHT GRAY\n",
        "def show(qmaze: Qmaze):\n",
        "    plt.grid(\"on\")\n",
        "    nrows, ncols = qmaze.maze.shape\n",
        "    ax = plt.gca()\n",
        "    ax.set_xticks(np.arange(0.5, nrows, 1))\n",
        "    ax.set_yticks(np.arange(0.5, ncols, 1))\n",
        "    ax.set_xticklabels([])\n",
        "    ax.set_yticklabels([])\n",
        "    canvas = np.copy(qmaze.maze)\n",
        "    for row, col in qmaze.visited:\n",
        "        canvas[row, col] = 0.6\n",
        "    mouse_row, mouse_col, _ = qmaze.state\n",
        "    canvas[mouse_row, mouse_col] = 0.3  # mouse cell\n",
        "    canvas[nrows - 1, ncols - 1] = 0.9  # cheese cell\n",
        "    img = plt.imshow(canvas, interpolation=\"none\", cmap=\"gray\")\n",
        "    return img"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate Random Maze\n",
        "\n",
        "Applying Depth First Search (DFS) to generate random maze based on entrance and exit (aka cheese) location adapted from https://www.geeksforgeeks.org/random-acyclic-maze-generator-with-given-entry-and-exit-point/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8WZ-PWIUTe3",
        "outputId": "6ab0e5b0-d016-4ea2-d1fc-605fd23bed79"
      },
      "outputs": [],
      "source": [
        "# Class to define structure of a node\n",
        "class Node:\n",
        "    def __init__(self, value=None, next_element=None):\n",
        "        self.val = value\n",
        "        self.next = next_element\n",
        "\n",
        "\n",
        "# Class to implement a stack\n",
        "class stack:\n",
        "    # Constructor\n",
        "    def __init__(self):\n",
        "        self.head = None\n",
        "        self.length = 0\n",
        "\n",
        "    # Put an item on the top of the stack\n",
        "    def insert(self, data):\n",
        "        self.head = Node(data, self.head)\n",
        "        self.length += 1\n",
        "\n",
        "    # Return the top position of the stack\n",
        "    def pop(self):\n",
        "        if self.length == 0:\n",
        "            return None\n",
        "        else:\n",
        "            returned = self.head.val\n",
        "            self.head = self.head.next\n",
        "            self.length -= 1\n",
        "            return returned\n",
        "\n",
        "    # Return False if the stack is empty\n",
        "    # and true otherwise\n",
        "    def not_empty(self):\n",
        "        return bool(self.length)\n",
        "\n",
        "    # Return the top position of the stack\n",
        "    def top(self):\n",
        "        return self.head.val\n",
        "\n",
        "\n",
        "def generate_random_maze(\n",
        "    rows: int = 8,\n",
        "    columns: int = 8,\n",
        "    initial_point: list = (0, 0),\n",
        "    final_point: list = (7, 7),\n",
        "):\n",
        "    ROWS, COLS = rows, columns\n",
        "\n",
        "    # Array with only walls (where paths will\n",
        "    # be created)\n",
        "    maze = list(list(0 for _ in range(COLS)) for _ in range(ROWS))\n",
        "\n",
        "    # Auxiliary matrices to avoid cycles\n",
        "    seen = list(list(False for _ in range(COLS)) for _ in range(ROWS))\n",
        "    previous = list(list((-1, -1) for _ in range(COLS)) for _ in range(ROWS))\n",
        "\n",
        "    S = stack()\n",
        "\n",
        "    # Insert initial position\n",
        "    S.insert(initial_point)\n",
        "\n",
        "    # Keep walking on the graph using dfs\n",
        "    # until we have no more paths to traverse\n",
        "    # (create)\n",
        "    while S.not_empty():\n",
        "        # Remove the position of the Stack\n",
        "        # and mark it as seen\n",
        "        x, y = S.pop()\n",
        "        seen[x][y] = True\n",
        "\n",
        "        # This is to avoid cycles with adj positions\n",
        "        if (x + 1 < ROWS) and maze[x + 1][y] == 1 and previous[x][y] != (x + 1, y):\n",
        "            continue\n",
        "        if (0 < x) and maze[x - 1][y] == 1 and previous[x][y] != (x - 1, y):\n",
        "            continue\n",
        "        if (y + 1 < COLS) and maze[x][y + 1] == 1 and previous[x][y] != (x, y + 1):\n",
        "            continue\n",
        "        if (y > 0) and maze[x][y - 1] == 1 and previous[x][y] != (x, y - 1):\n",
        "            continue\n",
        "\n",
        "        # Mark as walkable position\n",
        "        maze[x][y] = 1\n",
        "\n",
        "        # Array to shuffle neighbours before\n",
        "        # insertion\n",
        "        to_stack = []\n",
        "\n",
        "        # Before inserting any position,\n",
        "        # check if it is in the boundaries of\n",
        "        # the maze\n",
        "        # and if it were seen (to avoid cycles)\n",
        "\n",
        "        # If adj position is valid and was not seen yet\n",
        "        if (x + 1 < ROWS) and seen[x + 1][y] == False:\n",
        "            # Mark the adj position as seen\n",
        "            seen[x + 1][y] = True\n",
        "\n",
        "            # Memorize the position to insert the\n",
        "            # position in the stack\n",
        "            to_stack.append((x + 1, y))\n",
        "\n",
        "            # Memorize the current position as its\n",
        "            # previous position on the path\n",
        "            previous[x + 1][y] = (x, y)\n",
        "\n",
        "        if (0 < x) and seen[x - 1][y] == False:\n",
        "            # Mark the adj position as seen\n",
        "            seen[x - 1][y] = True\n",
        "\n",
        "            # Memorize the position to insert the\n",
        "            # position in the stack\n",
        "            to_stack.append((x - 1, y))\n",
        "\n",
        "            # Memorize the current position as its\n",
        "            # previous position on the path\n",
        "            previous[x - 1][y] = (x, y)\n",
        "\n",
        "        if (y + 1 < COLS) and seen[x][y + 1] == False:\n",
        "            # Mark the adj position as seen\n",
        "            seen[x][y + 1] = True\n",
        "\n",
        "            # Memorize the position to insert the\n",
        "            # position in the stack\n",
        "            to_stack.append((x, y + 1))\n",
        "\n",
        "            # Memorize the current position as its\n",
        "            # previous position on the path\n",
        "            previous[x][y + 1] = (x, y)\n",
        "\n",
        "        if (y > 0) and seen[x][y - 1] == False:\n",
        "            # Mark the adj position as seen\n",
        "            seen[x][y - 1] = True\n",
        "\n",
        "            # Memorize the position to insert the\n",
        "            # position in the stack\n",
        "            to_stack.append((x, y - 1))\n",
        "\n",
        "            # Memorize the current position as its\n",
        "            # previous position on the path\n",
        "            previous[x][y - 1] = (x, y)\n",
        "\n",
        "        # Indicates if Pf is a neighbour position\n",
        "        pf_flag = False\n",
        "        while len(to_stack):\n",
        "            # Remove random position\n",
        "            neighbour = to_stack.pop(randint(0, len(to_stack) - 1))\n",
        "\n",
        "            # Is the final position,\n",
        "            # remember that by marking the flag\n",
        "            if neighbour == final_point:\n",
        "                pf_flag = True\n",
        "\n",
        "            # Put on the top of the stack\n",
        "            else:\n",
        "                S.insert(neighbour)\n",
        "\n",
        "        # This way, Pf will be on the top\n",
        "        if pf_flag:\n",
        "            S.insert(final_point)\n",
        "\n",
        "    # Mark the initial position\n",
        "    x0, y0 = initial_point\n",
        "    xf, yf = final_point\n",
        "    maze[x0][y0] = 1\n",
        "    maze[xf][yf] = 1\n",
        "\n",
        "    # Return maze formed by the traversed path\n",
        "    return np.asarray(maze, dtype=\"float\")\n",
        "\n",
        "\n",
        "# Test Run to ensure that function is working correctly\n",
        "test_cols = 8\n",
        "test_rows = 8\n",
        "test_init_point = (0, 0)\n",
        "test_final_point = (7, 7)\n",
        "\n",
        "test_maze = generate_random_maze(\n",
        "    rows=test_rows,\n",
        "    columns=test_cols,\n",
        "    initial_point=test_init_point,\n",
        "    final_point=test_final_point,\n",
        ")\n",
        "# check that the shape generated is correct\n",
        "assert test_maze.shape == (test_cols, test_rows)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generating a maze array and initializing a Qmaze\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "EdFkyj_YGCEE",
        "outputId": "fd51aaac-dedd-41e4-c9f1-a37f5a22c35e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x12e087640>"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "maze = generate_random_maze(8, 8, (0, 0), (7, 7))\n",
        "qmaze = Qmaze(maze=maze)\n",
        "show(qmaze)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Model(object):\n",
        "    def __init__(self, maze, learning_rate: float = 0.001):\n",
        "        model = Sequential()\n",
        "        model.add(Dense(maze.size, input_shape=(maze.size,)))\n",
        "        model.add(PReLU())\n",
        "        model.add(Dense(maze.size))\n",
        "        model.add(PReLU())\n",
        "        model.add(Dense(num_actions))\n",
        "        model.compile(optimizer=\"adam\", loss=\"mse\")\n",
        "        self.model = model\n",
        "\n",
        "    def get_model(self):\n",
        "        return self.model"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create a Trial\n",
        "\n",
        "Create an `Trial` class that accepts a trained neural network which calculates the next action, a Qmaze and the initial cell that the mouse is in.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Trial:\n",
        "    def __init__(self, model: Model, qmaze: Qmaze, mouse_cell: list):\n",
        "        self._qmaze = qmaze\n",
        "        self._model = model\n",
        "        self.mouse_cell = mouse_cell\n",
        "\n",
        "    def run(self):\n",
        "        qmaze = self._get_maze()\n",
        "        model = self._get_model()\n",
        "        mouse_cell = self._get_mouse_cell()\n",
        "        qmaze.reset(mouse_cell)\n",
        "        env_state = qmaze.observe()\n",
        "        while True:\n",
        "            prev_env_state = env_state\n",
        "            Q = model.predict(prev_env_state)\n",
        "            action = np.argmax(Q[0])\n",
        "            _, _, status = qmaze.act(action)\n",
        "            if status == \"win\":\n",
        "                return True\n",
        "            elif status == \"lose\":\n",
        "                return False\n",
        "\n",
        "    # For small mazes we can allow ourselves to perform a completion check in which we simulate all possible\n",
        "    # games and check if our model wins the all. This is not practical for large mazes as it slows down training.\n",
        "    def check(self):\n",
        "        qmaze = self._get_maze()\n",
        "        for cell in qmaze.free_cells:\n",
        "            if not qmaze.valid_actions(cell):\n",
        "                return False\n",
        "            if not self.run():\n",
        "                return False\n",
        "        return True\n",
        "\n",
        "    def _get_maze(self):\n",
        "        return self._qmaze\n",
        "\n",
        "    def _get_model(self):\n",
        "        return self._model\n",
        "\n",
        "    def _get_mouse_cell(self):\n",
        "        return self.mouse_cell"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Creating a Class to Model the Experience of the Mouse\n",
        "\n",
        "Create an `Experience` class that collects the experience of `Experiments` within a `list` of memory. It retreives a `model`, a `max_memory` which defines the maximum amount of experiments that the mouse can _remember_ and a `discount` factor which represents the instantanious uncertainty in the _Bellman equation for stochastic environments_.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Experience(object):\n",
        "    def __init__(self, model: Model, max_memory: int = 100, discount: float = 95 / 100):\n",
        "        self.model = model\n",
        "        self.max_memory = max_memory\n",
        "        self.discount = discount\n",
        "        self.memory = list()\n",
        "        self.actions = model.get_model().output_shape[-1]\n",
        "\n",
        "    def remember(self, trial):\n",
        "        self.memory.append(trial)\n",
        "        if len(self.memory) > self.max_memory:\n",
        "            # delete the first element of the memory list if we exceed the max memory\n",
        "            del self.memory[0]\n",
        "\n",
        "    def predict(self, env_state):\n",
        "        return self.model.get_model().predict(env_state)[0]\n",
        "\n",
        "    def data(self, data_size: int = 10):\n",
        "        environment_size = self.memory[0][0].shape[1]\n",
        "        memory_size = len(self.memory)\n",
        "        data_size = min(memory_size, data_size)\n",
        "        inputs = np.zeros((data_size, environment_size))\n",
        "        targets = np.zeros((data_size, self.actions))\n",
        "        for idx, jdx in enumerate(\n",
        "            np.random.choice(range(memory_size), data_size, replace=False)\n",
        "        ):\n",
        "            envstate, action, reward, envstate_next, trial_over = self.memory[jdx]\n",
        "            inputs[idx] = envstate\n",
        "            # There should be no target values for actions not taken.\n",
        "            targets[idx] = self.predict(envstate)\n",
        "            # Q_sa = derived policy = max quality env/action = max_a' Q(s', a')\n",
        "            Q_sa = np.max(self.predict(envstate_next))\n",
        "            if trial_over:\n",
        "                targets[idx, action] = reward\n",
        "            else:\n",
        "                # reward + gamma * max_a' Q(s', a')\n",
        "                targets[idx, action] = reward + self.discount * Q_sa\n",
        "        return inputs, targets"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Q-Training Algorithm for Reinforcement Learning of Mouse\n",
        "The algorithm accepts the a `number_epoch` which is the number of epochs, the maximum memory `max_memory` which is the maximum number of trials kept in memory and the `data_size` which is the number of  samples in training epoch. This is the number of trials randomly selected from the mouse's experience"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This is a small utility for printing readable time strings:\n",
        "def format_time(seconds):\n",
        "    if seconds < 400:\n",
        "        s = float(seconds)\n",
        "        return \"%.1f seconds\" % (s,)\n",
        "    elif seconds < 4000:\n",
        "        m = seconds / 60.0\n",
        "        return \"%.2f minutes\" % (m,)\n",
        "    else:\n",
        "        h = seconds / 3600.0\n",
        "        return \"%.2f hours\" % (h,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Experiment(object):\n",
        "    def __init__(self, maze=generate_random_maze(), model_learning_rate: int = 0.001):\n",
        "        qmaze = Qmaze(maze)\n",
        "        model = Model(maze, learning_rate=model_learning_rate)\n",
        "        trial = Trial(model, qmaze, (0, 0))\n",
        "        self.qmaze = qmaze\n",
        "        self.model = model\n",
        "        self.trial = trial\n",
        "\n",
        "    def train(self, **opt):\n",
        "        global epsilon\n",
        "        number_epoch = opt.get(\"n_epoch\", 15000)\n",
        "        max_memory = opt.get(\"max_memory\", 1000)\n",
        "        data_size = opt.get(\"data_size\", 50)\n",
        "        weights_file = opt.get(\"weights_file\", \"\")\n",
        "        name = opt.get(\"name\", \"model\")\n",
        "        start_time = datetime.datetime.now()\n",
        "        if weights_file:\n",
        "            print(\"loading weights from file: %s\" % (weights_file,))\n",
        "            self.model.get_model().load_weights(weights_file)\n",
        "\n",
        "        # Initialize experience replay object\n",
        "        experience = Experience(self.model, max_memory=max_memory)\n",
        "\n",
        "        completion_history = []  # history of win/lose game\n",
        "        number_free_cells = len(qmaze.free_cells)\n",
        "        hsize = qmaze.maze.size // 2  # history window size\n",
        "        win_rate = 0.0\n",
        "        imctr = 1\n",
        "\n",
        "        for epoch in range(number_epoch):\n",
        "            loss = 0.0\n",
        "            mouse_cell = random.choice(qmaze.free_cells)\n",
        "            qmaze.reset(mouse_cell)\n",
        "            trial_over = False\n",
        "\n",
        "            # get initial envstate (1d flattened canvas)\n",
        "            envstate = qmaze.observe()\n",
        "\n",
        "            n_trials = 0\n",
        "            while not trial_over:\n",
        "                valid_actions = qmaze.valid_actions()\n",
        "                if not valid_actions:\n",
        "                    break\n",
        "                prev_envstate = envstate\n",
        "                # Get next action\n",
        "                if np.random.rand() < epsilon:\n",
        "                    action = random.choice(valid_actions)\n",
        "                else:\n",
        "                    action = np.argmax(experience.predict(prev_envstate))\n",
        "\n",
        "                # Apply action, get reward and new envstate\n",
        "                envstate, reward, status = qmaze.act(action)\n",
        "                if status == \"win\":\n",
        "                    completion_history.append(1)\n",
        "                    trial_over = True\n",
        "                elif status == \"lose\":\n",
        "                    completion_history.append(0)\n",
        "                    trial_over = True\n",
        "                else:\n",
        "                    trial_over = False\n",
        "\n",
        "                # Store trial (experience)\n",
        "                trial = [prev_envstate, action, reward, envstate, trial_over]\n",
        "                experience.remember(trial)\n",
        "                n_trials += 1\n",
        "\n",
        "                # Train neural network model\n",
        "                inputs, targets = experience.data(data_size=data_size)\n",
        "                _ = self.model.get_model().fit(\n",
        "                    inputs,\n",
        "                    targets,\n",
        "                    epochs=8,\n",
        "                    batch_size=16,\n",
        "                    verbose=0,\n",
        "                )\n",
        "                loss = self.model.get_model().evaluate(inputs, targets, verbose=0)\n",
        "\n",
        "            if len(completion_history) > hsize:\n",
        "                win_rate = sum(completion_history[-hsize:]) / hsize\n",
        "\n",
        "            dt = datetime.datetime.now() - start_time\n",
        "            t = format_time(dt.total_seconds())\n",
        "            template = \"Epoch: {:03d}/{:d} | Loss: {:.4f} | Trials: {:d} | Win count: {:d} | Win rate: {:.3f} | time: {}\"\n",
        "            print(\n",
        "                template.format(\n",
        "                    epoch,\n",
        "                    number_epoch - 1,\n",
        "                    loss,\n",
        "                    n_trials,\n",
        "                    sum(completion_history),\n",
        "                    win_rate,\n",
        "                    t,\n",
        "                )\n",
        "            )\n",
        "            # we simply check if training has exhausted all free cells and if in all\n",
        "            # cases the agent won\n",
        "            if win_rate > 0.9:\n",
        "                epsilon = 0.05\n",
        "            if sum(completion_history[-hsize:]) == hsize and self.trial.check():\n",
        "                print(\"Reached 100%% win rate at epoch: %d\" % (epoch,))\n",
        "                break\n",
        "\n",
        "        # Save trained model weights and architecture, this will be used by the visualization code\n",
        "        h5file = name + \".h5\"\n",
        "        json_file = name + \".json\"\n",
        "        self.model.get_model().save_weights(h5file, overwrite=True)\n",
        "        with open(json_file, \"w\") as outfile:\n",
        "            json.dump(self.model.get_model().to_json(), outfile)\n",
        "        end_time = datetime.datetime.now()\n",
        "        dt = datetime.datetime.now() - start_time\n",
        "        seconds = dt.total_seconds()\n",
        "        t = format_time(seconds)\n",
        "        print(\"files: %s, %s\" % (h5file, json_file))\n",
        "        print(\n",
        "            \"n_epoch: %d, max_mem: %d, data: %d, time: %s\"\n",
        "            % (epoch, max_memory, data_size, t)\n",
        "        )\n",
        "        return seconds"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 73ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m/Users/sammyrobens-paradise/projects/computational-neuroscience/project/reinforcement-mouse-learning.ipynb Cell 19\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sammyrobens-paradise/projects/computational-neuroscience/project/reinforcement-mouse-learning.ipynb#X25sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m experiment \u001b[39m=\u001b[39m Experiment()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/sammyrobens-paradise/projects/computational-neuroscience/project/reinforcement-mouse-learning.ipynb#X25sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m experiment\u001b[39m.\u001b[39;49mtrain(epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, max_memory\u001b[39m=\u001b[39;49m\u001b[39m8\u001b[39;49m \u001b[39m*\u001b[39;49m maze\u001b[39m.\u001b[39;49msize, data_size\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m)\n",
            "\u001b[1;32m/Users/sammyrobens-paradise/projects/computational-neuroscience/project/reinforcement-mouse-learning.ipynb Cell 19\u001b[0m in \u001b[0;36mExperiment.train\u001b[0;34m(self, **opt)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sammyrobens-paradise/projects/computational-neuroscience/project/reinforcement-mouse-learning.ipynb#X25sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m     \u001b[39m# Train neural network model\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sammyrobens-paradise/projects/computational-neuroscience/project/reinforcement-mouse-learning.ipynb#X25sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m     inputs, targets \u001b[39m=\u001b[39m experience\u001b[39m.\u001b[39mdata(data_size\u001b[39m=\u001b[39mdata_size)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/sammyrobens-paradise/projects/computational-neuroscience/project/reinforcement-mouse-learning.ipynb#X25sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m     _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mget_model()\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sammyrobens-paradise/projects/computational-neuroscience/project/reinforcement-mouse-learning.ipynb#X25sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m         inputs,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sammyrobens-paradise/projects/computational-neuroscience/project/reinforcement-mouse-learning.ipynb#X25sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m         targets,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sammyrobens-paradise/projects/computational-neuroscience/project/reinforcement-mouse-learning.ipynb#X25sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m         epochs\u001b[39m=\u001b[39;49m\u001b[39m8\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sammyrobens-paradise/projects/computational-neuroscience/project/reinforcement-mouse-learning.ipynb#X25sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m         batch_size\u001b[39m=\u001b[39;49m\u001b[39m16\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sammyrobens-paradise/projects/computational-neuroscience/project/reinforcement-mouse-learning.ipynb#X25sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m         verbose\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sammyrobens-paradise/projects/computational-neuroscience/project/reinforcement-mouse-learning.ipynb#X25sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sammyrobens-paradise/projects/computational-neuroscience/project/reinforcement-mouse-learning.ipynb#X25sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mget_model()\u001b[39m.\u001b[39mevaluate(inputs, targets, verbose\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sammyrobens-paradise/projects/computational-neuroscience/project/reinforcement-mouse-learning.ipynb#X25sZmlsZQ%3D%3D?line=78'>79</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(completion_history) \u001b[39m>\u001b[39m hsize:\n",
            "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
            "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/keras/engine/training.py:1685\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1677\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1678\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1679\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1682\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   1683\u001b[0m ):\n\u001b[1;32m   1684\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1685\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1686\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1687\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
            "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
            "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:894\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    891\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    893\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 894\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    896\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    897\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
            "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:926\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    923\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    924\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    925\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 926\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    927\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    928\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    929\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    930\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
            "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:144\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    141\u001b[0m   (concrete_function,\n\u001b[1;32m    142\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m    143\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39m_call_flat(\n\u001b[0;32m--> 144\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
            "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1961\u001b[0m, in \u001b[0;36mConcreteFunction.captured_inputs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1955\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[1;32m   1956\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcaptured_inputs\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m   1957\u001b[0m   \u001b[39m\"\"\"Returns external Tensors captured by this function.\u001b[39;00m\n\u001b[1;32m   1958\u001b[0m \n\u001b[1;32m   1959\u001b[0m \u001b[39m  self.__call__(*args) passes `args + self.captured_inputs` to the function.\u001b[39;00m\n\u001b[1;32m   1960\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1961\u001b[0m   \u001b[39mreturn\u001b[39;00m nest\u001b[39m.\u001b[39;49mflatten(\n\u001b[1;32m   1962\u001b[0m       [x() \u001b[39mif\u001b[39;49;00m callable(x) \u001b[39melse\u001b[39;49;00m x \u001b[39mfor\u001b[39;49;00m x \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_captured_inputs],\n\u001b[1;32m   1963\u001b[0m       expand_composites\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
            "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/tensorflow/python/util/nest.py:454\u001b[0m, in \u001b[0;36mflatten\u001b[0;34m(structure, expand_composites)\u001b[0m\n\u001b[1;32m    452\u001b[0m   \u001b[39mreturn\u001b[39;00m [\u001b[39mNone\u001b[39;00m]\n\u001b[1;32m    453\u001b[0m expand_composites \u001b[39m=\u001b[39m \u001b[39mbool\u001b[39m(expand_composites)\n\u001b[0;32m--> 454\u001b[0m \u001b[39mreturn\u001b[39;00m _pywrap_utils\u001b[39m.\u001b[39;49mFlatten(structure, expand_composites)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "experiment = Experiment()\n",
        "experiment.train(epochs=10, max_memory=8 * maze.size, data_size=32)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
